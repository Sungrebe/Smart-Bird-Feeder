{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sungrebe/Smart-Bird-Feeder/blob/main/cc_backyard_bird_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install astrapy and sentence_transformers (run this every time the runtime restarts)\n",
        "!pip install astrapy\n",
        "!pip install transformers\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from astrapy import DataAPIClient\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from google.colab import drive, userdata, files\n",
        "\n",
        "# load database and image collection\n",
        "\n",
        "client = DataAPIClient(userdata.get('ASTRA_DB_APPLICATION_TOKEN'))\n",
        "db = client.get_database(userdata.get('ASTRA_DB_API_ENDPOINT'))\n",
        "col = db.get_collection(\"cc_backyard_birds\")\n",
        "\n",
        "# load CLIP model (Vit-B-32)\n",
        "\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Helper method for the model to process image inputs\n",
        "def process_image(image_path):\n",
        "  input = processor(\n",
        "    images=Image.open(image_path),\n",
        "    return_tensors=\"pt\"\n",
        "  )\n",
        "  return model.get_image_features(**input).flatten()\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nSOwBwH8pTZM",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8119968-1441-4573-ccf8-4cb3d1173329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: astrapy in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: deprecation<2.2.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from astrapy) (2.1.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy) (0.28.1)\n",
            "Requirement already satisfied: pymongo>=3 in /usr/local/lib/python3.10/dist-packages (from astrapy) (4.10.1)\n",
            "Requirement already satisfied: toml<0.11.0,>=0.10.2 in /usr/local/lib/python3.10/dist-packages (from astrapy) (0.10.2)\n",
            "Requirement already satisfied: uuid6>=2024.1.12 in /usr/local/lib/python3.10/dist-packages (from astrapy) (2024.7.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from deprecation<2.2.0,>=2.1.0->astrapy) (24.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.2->httpx[http2]<1,>=0.25.2->astrapy) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.2->httpx[http2]<1,>=0.25.2->astrapy) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.2->httpx[http2]<1,>=0.25.2->astrapy) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.2->httpx[http2]<1,>=0.25.2->astrapy) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.25.2->httpx[http2]<1,>=0.25.2->astrapy) (0.14.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy) (4.1.0)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo>=3->astrapy) (2.7.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy) (6.0.1)\n",
            "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy) (4.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.25.2->httpx[http2]<1,>=0.25.2->astrapy) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.25.2->httpx[http2]<1,>=0.25.2->astrapy) (1.2.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hash table with select bird species (only 11 for testing purposes)\n",
        "# and their image folders from the nabirds dataset\n",
        "# in the final version we will have at least 100 species\n",
        "species = {\n",
        "    \"Mourning Dove\": [\"0529\"],\n",
        "    \"Red-bellied Woodpecker\": [\"0553\"],\n",
        "    \"Blue Jay\": [\"0950\"],\n",
        "    \"Black-capped Chickadee\": [\"0812\"],\n",
        "    \"Tufted Titmouse\": [\"0819\"],\n",
        "    \"European Starling\": [\"0748\", \"0856\", \"1005\"],\n",
        "    \"American Robin\": [\"0753\", \"0960\"],\n",
        "    \"House Finch\": [\"0790\", \"0997\"],\n",
        "    \"American Goldfinch\": [\"0794\", \"1001\"],\n",
        "    \"Common Grackle\": [\"0912\"],\n",
        "    \"Northern Cardinal\": [\"0772\", \"0979\"],\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "# load cc_backyard_birds images into dataset\n",
        "\n",
        "nabirds_dir = \"/content/drive/MyDrive/nabirds/\"\n",
        "\n",
        "for species_name, species_folders in species.items():\n",
        "  for folder in species_folders:\n",
        "    for img_file in os.listdir(nabirds_dir + folder):\n",
        "      # convert each image into a vector representation using CLIP model\n",
        "      img_vector = process_image(nabirds_dir + folder + \"/\" + img_file)\n",
        "\n",
        "      col.insert_one({\n",
        "          \"text\": species_name,\n",
        "          \"$vector\": img_vector.tolist(),\n",
        "      })\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DQOqGQigKSOD",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "89ce8ca3-6892-44f9-94e7-66295a86f16f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# load cc_backyard_birds images into dataset\\n\\nnabirds_dir = \"/content/drive/MyDrive/nabirds/\"\\n\\nfor species_name, species_folders in species.items():\\n  for folder in species_folders:\\n    for img_file in os.listdir(nabirds_dir + folder):\\n      # convert each image into a vector representation using CLIP model\\n      img_vector = process_image(nabirds_dir + folder + \"/\" + img_file)\\n\\n      col.insert_one({\\n          \"text\": species_name,\\n          \"$vector\": img_vector.tolist(),\\n      })\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: implement fine-tuning algorithm\n",
        "import random\n",
        "import os\n",
        "import re\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import shutil\n",
        "\n",
        "# We will use triplet loss\n",
        "\n",
        "# Basic idea is that for each species, we create a set of triplets\n",
        "# Each triplet consists of three images - an anchor, positive, and negative\n",
        "# The anchor image is an image of the species, the positive another\n",
        "# image of that species, and the negative an image of an unrelated but\n",
        "# easily confused species (from the perspective of the model)\n",
        "\n",
        "# 1. Create a dataset of triplets (~10 for each species to start with)\n",
        "# For each triplet, it will be saved in the following order:\n",
        "# the name of the bird species, then the image ID of the anchor\n",
        "# the image ID of the positive, and the image ID of the negative\n",
        "\n",
        "nabirds_dir = \"/content/drive/MyDrive/nabirds/\"\n",
        "\n",
        "async def create_triplets():\n",
        "  for species_name, species_folder in species.items():\n",
        "    anchors = random.sample(os.listdir(nabirds_dir + species_folder[0] + \"/\"), 10)\n",
        "\n",
        "    for i in range(len(anchors)):\n",
        "      shutil.copy2(nabirds_dir + species_folder[0] + \"/\" + anchors[i], nabirds_dir + f\"triplets/{species_name}/\" + f\"{i}_anchor\" + \".jpg\")\n",
        "\n",
        "      # get top results for each anchor image\n",
        "      results = await search_img(nabirds_dir + f\"triplets/{species_name}/\" + f\"{i}_anchor\" + \".jpg\")\n",
        "      results_list = list(results)\n",
        "\n",
        "      for result in results_list:\n",
        "        print(result['text'], result['$similarity'], result['_id'])\n",
        "\n",
        "      # lowest ranking correct result is pos, highest ranking incorrect result is neg\n",
        "      lowest_pos = list(filter(lambda x: x['text'] == species_name, reversed(results_list)))[0]\n",
        "      all_negatives = list(filter(lambda x: x['text'] != species_name, results_list))\n",
        "\n",
        "      if all_negatives != []:\n",
        "        highest_neg = all_negatives[0]\n",
        "\n",
        "      pos_id = lowest_pos['_id']\n",
        "      neg_id = highest_neg['_id']\n",
        "      neg_species = highest_neg['text']\n",
        "\n",
        "      # find file path of pos and neg images\n",
        "      pos_img_path = nabirds_dir + species_folder[0] + \"/\" + pos_id\n",
        "      neg_img_path = nabirds_dir + species[neg_species][0] + \"/\" + neg_id\n",
        "\n",
        "      shutil.copy2(pos_img_path, nabirds_dir + \"triplets/\" + species_name + \"/\" + f\"{i}_pos\" + \".jpg\")\n",
        "      shutil.copy2(neg_img_path, nabirds_dir + \"triplets/\" + species_name + \"/\" + f\"{i}_neg\" + \".jpg\")\n",
        "\n",
        "# 2. Split dataset into training, test, and validation\n",
        "# create subdirectories within the triplets folder containing those triplets\n",
        "# for training, test, and validation respectively\n",
        "\n",
        "def split_dataset():\n",
        "  # code goes here\n",
        "  for species_name, species_folder in species.items():\n",
        "    file_path = \"/content/drive/MyDrive/nabirds/triplets/\"\n",
        "    file_path += species_name\n",
        "    file_path += \"/\"\n",
        "\n",
        "    random_number = random.randint(0, 9)\n",
        "    if os.path.exists(file_path):\n",
        "      os.makedirs(file_path + \"train\")\n",
        "      os.makedirs(file_path + \"test\")\n",
        "      os.makedirs(file_path + \"validation\")\n",
        "      for i in range(10):\n",
        "        random_number = (random_number + 1) % 10\n",
        "        if i < 8:\n",
        "          shutil.move(file_path + f\"{i}_pos.jpg\", file_path + \"train/\" + f\"{i}_pos.jpg\")\n",
        "          shutil.move(file_path + f\"{i}_neg.jpg\", file_path + \"train/\" + f\"{i}_neg.jpg\")\n",
        "          shutil.move(file_path + f\"{i}_anchor.jpg\", file_path + \"train/\" + f\"{i}_anchor.jpg\")\n",
        "        if i == 8:\n",
        "          shutil.move(file_path + f\"{i}_pos.jpg\", file_path + \"test/\" + f\"{i}_pos.jpg\")\n",
        "          shutil.move(file_path + f\"{i}_neg.jpg\", file_path + \"test/\" + f\"{i}_neg.jpg\")\n",
        "          shutil.move(file_path + f\"{i}_anchor.jpg\", file_path + \"test/\" + f\"{i}_anchor.jpg\")\n",
        "        if i > 8:\n",
        "          shutil.move(file_path + f\"{i}_pos.jpg\", file_path + \"validation/\" + f\"{i}_pos.jpg\")\n",
        "          shutil.move(file_path + f\"{i}_neg.jpg\", file_path + \"validation/\" + f\"{i}_neg.jpg\")\n",
        "          shutil.move(file_path + f\"{i}_anchor.jpg\", file_path + \"validation/\" + f\"{i}_anchor.jpg\")\n",
        "    else:\n",
        "      print(f\"The file '{file_path}' does not exist.\")\n",
        "\n",
        "\n",
        "split_dataset()\n",
        "\n",
        "# 3. Define triplet loss function\n",
        "\n",
        "margin = 1.0\n",
        "\n",
        "def triplet_loss():\n",
        "  # code goes here\n",
        "  tloss_species = {}\n",
        "  for species_name, species_folder in species.items():\n",
        "    file_path = \"/content/drive/MyDrive/nabirds/triplets/\"\n",
        "    file_path += species_name\n",
        "    file_path += \"/train/\"\n",
        "    total_loss = 0\n",
        "    for i in range(8):\n",
        "      if (os.path.exists(file_path + f\"{i}_pos.jpg\")):\n",
        "        anchor = process_image(file_path + f\"{i}_anchor.jpg\")\n",
        "        positive = process_image(file_path + f\"{i}_pos.jpg\")\n",
        "        negative = process_image(file_path + f\"{i}_neg.jpg\")\n",
        "\n",
        "        pos_distance = F.pairwise_distance(anchor, positive, p=2)\n",
        "        neg_distance = F.pairwise_distance(anchor, negative, p=2)\n",
        "\n",
        "        loss = torch.clamp(pos_distance - neg_distance + margin, min=0.0)\n",
        "        total_loss += loss\n",
        "    tloss_species[species_name] = total_loss.item() / 8\n",
        "    file_path = os.path.join(file_path, 'loss.txt')\n",
        "    with open(file_path, 'w') as f:\n",
        "      f.write(str(total_loss.item() / 8))\n",
        "  return tloss_species\n",
        "\n",
        "#triplet_loss = triplet_loss()\n",
        "#print(triplet_loss)\n",
        "\n",
        "# 4. Write a training loop\n",
        "  # - Compute predicted outputs\n",
        "  # - Get loss for each step\n",
        "  # - Perform optimization\n",
        "  # - Get val accuracy after each step\n",
        "\n",
        "# 5. Save a copy of the fine tuned model\n",
        "\n",
        "# 6. Create a new collection of images using the fine tuned model and\n",
        "# perform image searches using that collection\n",
        "\n",
        "\"\"\"\n",
        "# 2. Split dataset into training, test, and validation\n",
        "async def split_dataset(triplets):\n",
        "  training_triplets = []\n",
        "  test_triplets = []\n",
        "  validation_triplets = []\n",
        "  i = 0\n",
        "  for triplet in triplets:\n",
        "    if (i % 10 < 8):\n",
        "      training_triplets.append(triplet)\n",
        "    elif (i % 10 == 8):\n",
        "      test_triplets.append(triplet)\n",
        "    else:\n",
        "      validation_triplets.append(triplet)\n",
        "    i += 1\n",
        "  return training_triplets, test_triplets, validation_triplets\n",
        "\n",
        "training_triplets, test_triplets, validation_triplets = await split_dataset(triplets)\n",
        "\n",
        "# 3. Define triplet loss function\n",
        "# Torch import at the top\n",
        "\n",
        "def triplet_loss(anchor, positive, negative, margin=1.0):\n",
        "    \"\"\"\n",
        "\"\"\"\n",
        "    Compute the triplet loss according to the formula:\n",
        "    Loss = max(d(A, P) - d(A, N) + margin, 0)\n",
        "\n",
        "    Parameters:\n",
        "    - anchor: address for anchor, needs to be converted to tensor of shape (batch_size, embedding_dim) for the anchor examples\n",
        "    - positive: address for positive, needs to be converted to tensor of shape (batch_size, embedding_dim) for the positive examples\n",
        "    - negative: address for negative, needs to be converted to tensor of shape (batch_size, embedding_dim) for the negative examples\n",
        "    - margin: Float, the margin for triplet loss\n",
        "\n",
        "    Returns:\n",
        "    - loss: Scalar, the computed triplet loss\n",
        "\n",
        "    anchor_tensor = process_image(nabirds_dir + \"/\" + anchor)\n",
        "    positive_tensor = process_image(nabirds_dir + \"/\" + positive)\n",
        "    negative_tensor = process_image(nabirds_dir + \"/\" + negative)\n",
        "    # Compute the pairwise distances\n",
        "    pos_distance = F.pairwise_distance(anchor_tensor, positive_tensor, p=2)\n",
        "    neg_distance = F.pairwise_distance(anchor_tensor, negative_tensor, p=2)\n",
        "\n",
        "    # Compute the triplet loss\n",
        "    loss = torch.clamp(pos_distance - neg_distance + margin, min=0.0)\n",
        "\n",
        "    # Return the mean loss over the batch\n",
        "    return loss.mean()\n",
        "\n",
        "for triplet in training_triplets:\n",
        "  print(triplet)\n",
        "  print(triplet_loss(triplet[1], triplet[2], triplet[3]))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qbUP0aSvkNXj",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "c5d8f879-b4c6-493a-d016-5b31b8dc282d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Mourning Dove': 0.6201287508010864, 'Red-bellied Woodpecker': 0.5542886853218079, 'Blue Jay': 1.3797094821929932, 'Black-capped Chickadee': 0.6545658707618713, 'Tufted Titmouse': 0.5019367933273315, 'European Starling': 1.3907968997955322, 'American Robin': 0.8361013531684875, 'House Finch': 0.8172570466995239, 'American Goldfinch': 0.0, 'Common Grackle': 1.2740358114242554, 'Northern Cardinal': 0.18613117933273315}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    Compute the triplet loss according to the formula:\\n    Loss = max(d(A, P) - d(A, N) + margin, 0)\\n\\n    Parameters:\\n    - anchor: address for anchor, needs to be converted to tensor of shape (batch_size, embedding_dim) for the anchor examples\\n    - positive: address for positive, needs to be converted to tensor of shape (batch_size, embedding_dim) for the positive examples\\n    - negative: address for negative, needs to be converted to tensor of shape (batch_size, embedding_dim) for the negative examples\\n    - margin: Float, the margin for triplet loss\\n\\n    Returns:\\n    - loss: Scalar, the computed triplet loss\\n\\n    anchor_tensor = process_image(nabirds_dir + \"/\" + anchor)\\n    positive_tensor = process_image(nabirds_dir + \"/\" + positive)\\n    negative_tensor = process_image(nabirds_dir + \"/\" + negative)\\n    # Compute the pairwise distances\\n    pos_distance = F.pairwise_distance(anchor_tensor, positive_tensor, p=2)\\n    neg_distance = F.pairwise_distance(anchor_tensor, negative_tensor, p=2)\\n\\n    # Compute the triplet loss\\n    loss = torch.clamp(pos_distance - neg_distance + margin, min=0.0)\\n\\n    # Return the mean loss over the batch\\n    return loss.mean()\\n\\nfor triplet in training_triplets:\\n  print(triplet)\\n  print(triplet_loss(triplet[1], triplet[2], triplet[3]))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKHN4YD8pE1j",
        "outputId": "7a5840d5-fa25-4ee3-8f2b-6e6e42547fea",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function triplet_loss at 0x7929f240af80>\n"
          ]
        }
      ],
      "source": [
        "# method for searching database\n",
        "# returns top 15 most similar images given a specific image\n",
        "\n",
        "async def search_img(image_path):\n",
        "  searching_vector = process_image(image_path).tolist()\n",
        "  most_similar_imgs = col.find(\n",
        "      sort={\"$vector\": searching_vector},\n",
        "      limit=30,\n",
        "      include_similarity=True,\n",
        "  )\n",
        "\n",
        "  return most_similar_imgs\n",
        "\n",
        "# demo code, loads a user inputted image and classifies it using the search_img method\n",
        "\n",
        "images = files.upload()\n",
        "input_path = next(iter(images))\n",
        "\n",
        "plt.imshow(Image.open(input_path))\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "results = await search_img(input_path)\n",
        "\n",
        "for result in results:\n",
        "  print(result['text'], result['$similarity'])"
      ]
    }
  ]
}